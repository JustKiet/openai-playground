{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "725590cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openai pydantic python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaad78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from functools import update_wrapper\n",
    "from typing import Callable, Any, Optional, Dict, TypeVar, overload, Union, cast\n",
    "from pydantic import create_model\n",
    "from openai.types.chat import ChatCompletionToolUnionParam\n",
    "import inspect\n",
    "\n",
    "def build_tool_schema(\n",
    "    func: Callable[..., Any], \n",
    "    name: str,\n",
    "    description: str = \"\"\n",
    ") -> ChatCompletionToolUnionParam:\n",
    "    signature = inspect.signature(func)\n",
    "    final_description = inspect.getdoc(func) or description\n",
    "\n",
    "    model_fields = {\n",
    "        name: (param.annotation, ...)\n",
    "        for name, param in signature.parameters.items()\n",
    "    }\n",
    "\n",
    "    ToolArguments = create_model(\"ToolArguments\", **model_fields) # type: ignore\n",
    "    raw_schema = ToolArguments.model_json_schema() # type: ignore\n",
    "    raw_schema.pop(\"title\", None) # type: ignore\n",
    "    raw_schema[\"additionalProperties\"] = False\n",
    "\n",
    "    return {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": name,\n",
    "            \"description\": final_description,\n",
    "            \"strict\": True,\n",
    "            \"parameters\": raw_schema,\n",
    "        },\n",
    "    }\n",
    "\n",
    "class Tool:\n",
    "    \"\"\"\n",
    "    Wrapper that makes a function into a tool with a schema.\n",
    "    \"\"\"\n",
    "    __tool_wrapped__ = True\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        func: Optional[Callable[..., Any]] = None,\n",
    "        *,\n",
    "        schema: Optional[ChatCompletionToolUnionParam] = None,\n",
    "    ) -> None:\n",
    "        if func is None:\n",
    "            func = self.__call__\n",
    "\n",
    "        self._func = func\n",
    "\n",
    "        if schema is None:\n",
    "            schema = build_tool_schema(self._func, name=str(self.__class__.__name__))\n",
    "        \n",
    "        self.schema = schema\n",
    "        update_wrapper(self, self._func)\n",
    "\n",
    "    @classmethod\n",
    "    def from_decorator(\n",
    "        cls,\n",
    "        func: Callable[..., Any],\n",
    "        *,\n",
    "        schema: ChatCompletionToolUnionParam,\n",
    "    ) -> \"Tool\":\n",
    "        \"\"\" \n",
    "        Create a Tool instance from a function and a schema.\n",
    "        This is used internally by the `tool` decorator.\n",
    "        \"\"\"\n",
    "        tool_instance = cls(func, schema=schema)\n",
    "        return tool_instance\n",
    "\n",
    "    def __call__(self, *args: Any, **kwargs: Any) -> Any:\n",
    "        if inspect.iscoroutinefunction(self._func):\n",
    "            return self._func(*args, **kwargs)\n",
    "        return self._func(*args, **kwargs)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"<Tool {self._func.__name__}>\"\n",
    "    \n",
    "    def __name__(self) -> str:\n",
    "        return self._func.__name__\n",
    "\n",
    "T = TypeVar(\"T\", bound=Callable[..., Any])\n",
    "\n",
    "@overload\n",
    "def tool(func: T) -> Tool: ... # type: ignore\n",
    "\n",
    "@overload\n",
    "def tool(\n",
    "    *,\n",
    "    description: str = \"\",\n",
    ") -> Callable[[T], Tool]: ... # type: ignore\n",
    "\n",
    "def tool(\n",
    "    func: Optional[T] = None,\n",
    "    *,\n",
    "    description: str = \"\",\n",
    ") -> Union[Tool, Callable[[T], Tool]]:\n",
    "    \"\"\"\n",
    "    Decorator to wrap a function into a Tool with OpenAI function-calling schema.\n",
    "    \"\"\"\n",
    "    def decorator(inner_func: T) -> Tool:\n",
    "        # Inspect signature and build pydantic model for parameters\n",
    "        signature = inspect.signature(inner_func)\n",
    "        final_description = inspect.getdoc(inner_func) or description\n",
    "\n",
    "        model_fields = {\n",
    "            name: (param.annotation, ...)\n",
    "            for name, param in signature.parameters.items()\n",
    "        }\n",
    "        ToolArguments = create_model(\"ToolArguments\", **model_fields)  # type: ignore\n",
    "        raw_schema = ToolArguments.model_json_schema() # type: ignore\n",
    "        assert isinstance(raw_schema, dict), \"ToolArguments.model_json_schema() must return a dict\"\n",
    "        tool_arguments: Dict[str, Any] = raw_schema # type: ignore\n",
    "        tool_arguments.pop(\"title\", None)\n",
    "        tool_arguments[\"additionalProperties\"] = False\n",
    "\n",
    "        schema: Any = {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": inner_func.__name__,\n",
    "                \"description\": final_description,\n",
    "                \"strict\": True,\n",
    "                \"parameters\": tool_arguments,\n",
    "            },\n",
    "        }\n",
    "\n",
    "        schema = cast(ChatCompletionToolUnionParam, schema)\n",
    "\n",
    "        return Tool.from_decorator(inner_func, schema=schema)\n",
    "\n",
    "    # If used without args: @tool\n",
    "    return decorator if func is None else decorator(func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b5ea8374",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import Generator, Callable, Any\n",
    "from pydantic.dataclasses import dataclass\n",
    "from openai.types.chat import ChatCompletionMessageParam\n",
    "from enum import Enum\n",
    "import json\n",
    "\n",
    "MessageLike = Union[Dict[str, Any], ChatCompletionMessageParam]\n",
    "\n",
    "class StreamingEventType(Enum):\n",
    "    TOKEN = \"token\"\n",
    "    TOOL_CALL = \"tool_call\"\n",
    "    TOOL_RESULT = \"tool_result\"\n",
    "    COMPLETED = \"completed\"\n",
    "\n",
    "class AgentStreamingEventType(Enum):\n",
    "    TOKEN = \"token\"\n",
    "    ACTION = \"action\"\n",
    "    ACTION_RESULT = \"action_result\"\n",
    "    COMPLETED = \"completed\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ToolCall:\n",
    "    name: str\n",
    "    tool_call_id: str\n",
    "    arguments: dict[str, Any]\n",
    "\n",
    "@dataclass\n",
    "class Context:\n",
    "    conversation_id: str\n",
    "    user_query: str\n",
    "    messages: list[MessageLike]\n",
    "    tools: list[Callable[..., str]]\n",
    "\n",
    "@dataclass\n",
    "class LLMStreamingEvent:\n",
    "    event: StreamingEventType\n",
    "    data: str\n",
    "    tool_call_id: str | None = None\n",
    "    tool_calls: list[ToolCall] | None = None\n",
    "\n",
    "@dataclass\n",
    "class AgentStreamingEvent:\n",
    "    event: AgentStreamingEventType\n",
    "    data: str\n",
    "    action_id: str | None = None\n",
    "\n",
    "class LLM(ABC):\n",
    "    @abstractmethod\n",
    "    def generate_text(self, messages: list[MessageLike], tools: Optional[list[Callable[..., str]]] = None) -> Generator[LLMStreamingEvent, None, None]:\n",
    "        \"\"\"Generate text based on the given prompt.\"\"\"\n",
    "        pass\n",
    "\n",
    "class EmbeddingModel(ABC):\n",
    "    @abstractmethod\n",
    "    def embed_texts(self, texts: list[str]) -> list[list[float]]:\n",
    "        \"\"\"Generate embedding for the given texts.\"\"\"\n",
    "        pass\n",
    "\n",
    "class ToolHandler:\n",
    "    def __init__(self, tools: list[Callable[..., str]]) -> None:\n",
    "        super().__init__()\n",
    "        self.tool_mappings: dict[str, Callable[..., str]] = {}\n",
    "        self.add_tools(tools)\n",
    "\n",
    "    def add_tools(self, tools: list[Callable[..., str]]) -> None:\n",
    "        \"\"\"Add a tool to the router.\"\"\"\n",
    "        for tool in tools:\n",
    "            if tool.__name__ in self.tool_mappings:\n",
    "                continue\n",
    "            self.tool_mappings[tool.__name__] = tool\n",
    "\n",
    "    def execute_tool(self, tool_call: ToolCall) -> str:\n",
    "        \"\"\"Execute the specified tool call.\"\"\"\n",
    "        tool = self.tool_mappings.get(tool_call.name)\n",
    "        if tool:\n",
    "            return tool(**tool_call.arguments)\n",
    "        return \"Tool not found.\"\n",
    "    \n",
    "class ToolRouter(ABC):\n",
    "    def __init__(self, tool_handler: ToolHandler) -> None:\n",
    "        super().__init__()\n",
    "        self.tool_handler = tool_handler\n",
    "\n",
    "    def add_tools(self, tools: list[Callable[..., str]]) -> None:\n",
    "        \"\"\"Add tools to the router.\"\"\"\n",
    "        self.tool_handler.add_tools(tools)        \n",
    "\n",
    "    @abstractmethod\n",
    "    def retrieve(self, query: str) -> list[Callable[..., str]]:\n",
    "        \"\"\"Retrieve appropriate tools based on the query.\"\"\"\n",
    "        pass\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, llm: LLM, tool_handler: ToolHandler) -> None:\n",
    "        super().__init__()\n",
    "        self.llm = llm\n",
    "        self.tool_handler = tool_handler\n",
    "        self._tools: list[Callable[..., str]] = []\n",
    "\n",
    "    def add_tools(self, tools: list[Callable[..., str]]) -> None:\n",
    "        \"\"\"Add tools to the agent.\"\"\"\n",
    "        self._tools.extend(tools)\n",
    "        self.tool_handler.add_tools(tools)\n",
    "\n",
    "    def run(self, messages: list[MessageLike]) -> Generator[LLMStreamingEvent, None, None]:\n",
    "        \"\"\"Run the agent with the given prompt.\"\"\"\n",
    "        while True:\n",
    "            print(\"Agent running with messages:\", messages)\n",
    "            for delta in self.llm.generate_text(messages, tools=self._tools):\n",
    "                if delta.event == StreamingEventType.TOOL_CALL and delta.tool_calls:\n",
    "                    yield delta\n",
    "                    for tool_call in delta.tool_calls:\n",
    "                        print(f\"Agent executing tool: {tool_call.name} with arguments {tool_call.arguments}\")\n",
    "                        messages.append({\"role\": \"assistant\", \"tool_calls\": [{\"id\": tool_call.tool_call_id, \"type\": \"function\", \"function\": {\"name\": tool_call.name, \"arguments\": json.dumps(tool_call.arguments)}}]})\n",
    "                        tool_result = self.tool_handler.execute_tool(tool_call)\n",
    "                        yield LLMStreamingEvent(\n",
    "                            event=StreamingEventType.TOOL_RESULT,\n",
    "                            tool_call_id=tool_call.tool_call_id,\n",
    "                            data=tool_result,\n",
    "                        )\n",
    "                        messages.append({\"role\": \"tool\", \"tool_call_id\": tool_call.tool_call_id, \"content\": tool_result})\n",
    "                elif delta.event == StreamingEventType.COMPLETED:\n",
    "                    yield LLMStreamingEvent(\n",
    "                        event=StreamingEventType.COMPLETED,\n",
    "                        data=delta.data,\n",
    "                    )\n",
    "                    return\n",
    "                yield LLMStreamingEvent(\n",
    "                    event=StreamingEventType.TOKEN,\n",
    "                    data=delta.data,\n",
    "                )\n",
    "\n",
    "class ConversationRepository(ABC):\n",
    "    @abstractmethod\n",
    "    def get_conversation_history(self, conversation_id: str) -> list[MessageLike]:\n",
    "        \"\"\"Retrieve the conversation history for the given conversation ID.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def add_messages(self, conversation_id: str, messages: list[MessageLike]) -> None:\n",
    "        \"\"\"Add messages to the conversation history.\"\"\"\n",
    "        pass\n",
    "\n",
    "class ContextMiddleware:\n",
    "    def __init__(\n",
    "        self, \n",
    "        conversation_repository: ConversationRepository,\n",
    "        tool_router: ToolRouter\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.conversation_repository = conversation_repository\n",
    "        self.tool_router = tool_router\n",
    "        \n",
    "    def prepare_context(self, conversation_id: str, query: str) -> Context:\n",
    "        \"\"\"Prepare context for the LLM based on conversation ID and query.\"\"\"\n",
    "        history = self.conversation_repository.get_conversation_history(conversation_id)\n",
    "        tools = self.tool_router.retrieve(query)\n",
    "\n",
    "        history.append({\"role\": \"user\", \"content\": query})\n",
    "        \n",
    "        return Context(\n",
    "            conversation_id=conversation_id,\n",
    "            user_query=query,\n",
    "            messages=history,\n",
    "            tools=tools\n",
    "        )\n",
    "\n",
    "class AgentService:\n",
    "    def __init__(\n",
    "        self, \n",
    "        agent: Agent,\n",
    "        context_middleware: ContextMiddleware\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.agent = agent\n",
    "        self.context_middleware = context_middleware\n",
    "\n",
    "    def handle_request(self, conversation_id: str, query: str) -> Generator[AgentStreamingEvent, None, None]:\n",
    "        \"\"\"Handle the request by preparing context and generating text.\"\"\"\n",
    "        context = self.context_middleware.prepare_context(conversation_id, query)\n",
    "        self.agent.add_tools(context.tools)\n",
    "        for delta in self.agent.run(context.messages):\n",
    "            if delta.event == StreamingEventType.TOOL_CALL and delta.tool_calls:\n",
    "                for tool_call in delta.tool_calls:\n",
    "                    yield AgentStreamingEvent(\n",
    "                        event=AgentStreamingEventType.ACTION,\n",
    "                        data=f\"Calling tool {tool_call.name} with arguments {tool_call.arguments}\",\n",
    "                        action_id=tool_call.tool_call_id or \"\"\n",
    "                    )\n",
    "            \n",
    "            elif delta.event == StreamingEventType.TOOL_RESULT:\n",
    "                yield AgentStreamingEvent(\n",
    "                    event=AgentStreamingEventType.ACTION_RESULT,\n",
    "                    data=delta.data,\n",
    "                    action_id=delta.tool_call_id or \"\"\n",
    "                )\n",
    "\n",
    "            elif delta.event == StreamingEventType.COMPLETED:\n",
    "                self.context_middleware.conversation_repository.add_messages(\n",
    "                    conversation_id,\n",
    "                    [{\"role\": \"user\", \"content\": query}, {\"role\": \"assistant\", \"content\": delta.data}]\n",
    "                )\n",
    "                yield AgentStreamingEvent(\n",
    "                    event=AgentStreamingEventType.COMPLETED,\n",
    "                    data=delta.data\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                yield AgentStreamingEvent(\n",
    "                    event=AgentStreamingEventType.TOKEN,\n",
    "                    data=delta.data\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1c3efaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai.types.chat import ChatCompletionMessageParam\n",
    "from openai.types.chat.chat_completion_chunk import ChoiceDeltaToolCall\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing import Sequence\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class OpenAILLM(LLM):\n",
    "    def __init__(self, model_name: openai.types.ChatModel, api_key: str | None = os.getenv(\"OPENAI_API_KEY\")) -> None:\n",
    "        self.model_name = model_name\n",
    "        self.api_key = api_key \n",
    "        self.client = openai.OpenAI(api_key=self.api_key)\n",
    "\n",
    "    def generate_text(self, messages: list[MessageLike], tools: Optional[list[Callable[..., str]]] = None) -> Generator[LLMStreamingEvent, None, None]:\n",
    "        openai_tools: Sequence[ChatCompletionToolUnionParam] = []\n",
    "        for t in tools or []:\n",
    "            wrapped_tool = tool(t)\n",
    "            openai_tools.append(wrapped_tool.schema)\n",
    "\n",
    "        print(f\"Generating text with model {self.model_name} and tools: {[openai_tool['function']['name'] for openai_tool in openai_tools]}\") # type: ignore\n",
    "        \n",
    "        generator = self.client.chat.completions.create(\n",
    "            model=self.model_name,\n",
    "            messages=cast(Sequence[ChatCompletionMessageParam], messages), \n",
    "            tools=openai_tools if openai_tools else openai.omit,\n",
    "            stream=True,\n",
    "        )\n",
    "        \n",
    "        full_content = \"\"\n",
    "        final_tool_calls: dict[int, ChoiceDeltaToolCall] = {}\n",
    "        for chunk in generator:\n",
    "            delta = chunk.choices[0].delta\n",
    "            \n",
    "            if delta.content:\n",
    "                full_content += delta.content\n",
    "                yield LLMStreamingEvent(\n",
    "                    event=StreamingEventType.TOKEN,\n",
    "                    data=delta.content\n",
    "                )\n",
    "            \n",
    "            if delta.tool_calls:\n",
    "                for tool_call in delta.tool_calls or []:\n",
    "                    \n",
    "                    index = tool_call.index\n",
    "\n",
    "                    if index not in final_tool_calls:\n",
    "                        final_tool_calls[index] = tool_call\n",
    "\n",
    "                    final_tool_calls[index].function.arguments += tool_call.function.arguments # type: ignore\n",
    "            \n",
    "            if chunk.choices[0].finish_reason == \"tool_calls\":\n",
    "                yield LLMStreamingEvent(\n",
    "                    event=StreamingEventType.TOOL_CALL,\n",
    "                    data=\"\",\n",
    "                    tool_calls=[ToolCall(\n",
    "                        name=tool_call.function.name or \"\" if tool_call.function else \"\",\n",
    "                        tool_call_id=tool_call.id or \"\",\n",
    "                        arguments=json.loads(tool_call.function.arguments or \"{}\" if tool_call.function else \"{}\")\n",
    "                    ) for tool_call in final_tool_calls.values()]\n",
    "                )\n",
    "            \n",
    "            if chunk.choices[0].finish_reason == \"stop\":\n",
    "                yield LLMStreamingEvent(\n",
    "                    event=StreamingEventType.COMPLETED,\n",
    "                    data=full_content\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d463787b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenAIEmbeddingModel(EmbeddingModel):\n",
    "    def __init__(self, model_name: openai.types.EmbeddingModel, api_key: str | None = os.getenv(\"OPENAI_API_KEY\")) -> None:\n",
    "        self.model_name = model_name\n",
    "        self.api_key = api_key \n",
    "        self.client = openai.OpenAI(api_key=self.api_key)\n",
    "\n",
    "    def embed_texts(self, texts: list[str]) -> list[list[float]]:\n",
    "        if not texts:\n",
    "            return []\n",
    "        print(f\"Generating embeddings given model {self.model_name} for texts: {texts}\")\n",
    "        response = self.client.embeddings.create(\n",
    "            model=self.model_name,\n",
    "            input=texts\n",
    "        )\n",
    "        embeddings = [data.embedding for data in response.data]\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6dae0f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1: list[float], vec2: list[float]) -> float:\n",
    "    dot_product = sum(a * b for a, b in zip(vec1, vec2))\n",
    "    magnitude1 = sum(a * a for a in vec1) ** 0.5\n",
    "    magnitude2 = sum(b * b for b in vec2) ** 0.5\n",
    "    if magnitude1 == 0 or magnitude2 == 0:\n",
    "        return 0.0\n",
    "    return dot_product / (magnitude1 * magnitude2)\n",
    "\n",
    "def elbow_method(similarities: list[float]) -> float:\n",
    "    if not similarities:\n",
    "        return 0.0\n",
    "\n",
    "    n = len(similarities)\n",
    "    if n < 3:\n",
    "        # if there are less than 3 points, return the minimum similarity\n",
    "        return min(similarities)\n",
    "\n",
    "    # Normalize indices to [0, 1] range for better distance calculation\n",
    "    points = [(i / (n - 1), sim) for i, sim in enumerate(similarities)]\n",
    "    start = points[0]\n",
    "    end = points[-1]\n",
    "\n",
    "    max_distance = -1.0\n",
    "    elbow_index = 0\n",
    "\n",
    "    # Calculate perpendicular distance from each point to the line connecting start and end\n",
    "    for i in range(1, n - 1):\n",
    "        point = points[i]\n",
    "        \n",
    "        # Vector from start to end\n",
    "        line_vec_x = end[0] - start[0]\n",
    "        line_vec_y = end[1] - start[1]\n",
    "        \n",
    "        # Vector from start to point\n",
    "        point_vec_x = point[0] - start[0]\n",
    "        point_vec_y = point[1] - start[1]\n",
    "        \n",
    "        # Calculate perpendicular distance using cross product formula\n",
    "        line_length_sq = line_vec_x ** 2 + line_vec_y ** 2\n",
    "        \n",
    "        if line_length_sq == 0:\n",
    "            distance = 0.0\n",
    "        else:\n",
    "            # Perpendicular distance = |cross product| / |line length|\n",
    "            cross_product = abs(line_vec_x * point_vec_y - line_vec_y * point_vec_x)\n",
    "            distance = cross_product / (line_length_sq ** 0.5)\n",
    "\n",
    "        if distance > max_distance:\n",
    "            max_distance = distance\n",
    "            elbow_index = i\n",
    "\n",
    "    return similarities[elbow_index]\n",
    "\n",
    "class EmbeddingBasedToolRouter(ToolRouter):\n",
    "    def __init__(self, tool_handler: ToolHandler, embedding_model: EmbeddingModel, threshold: float = 0.0) -> None:\n",
    "        super().__init__(tool_handler=tool_handler)\n",
    "        self.embedding_model = embedding_model\n",
    "        self.tool_embeddings: dict[str, list[float]] = {}\n",
    "        self.threshold = threshold\n",
    "        self.add_tools(list(tool_handler.tool_mappings.values()))\n",
    "        \n",
    "    def add_tools(self, tools: list[Callable[..., str]]) -> None:\n",
    "        \"\"\"Add a tool to the router and compute its embedding.\"\"\"\n",
    "        super().add_tools(tools)\n",
    "        tool_descriptions = [f\"{tool.__name__}: {tool.__doc__ or ''}\" for tool in tools]\n",
    "        tool_embeddings = self.embedding_model.embed_texts(tool_descriptions)\n",
    "        for tool, tool_embedding in zip(tools, tool_embeddings):\n",
    "            name = tool.__name__\n",
    "            self.tool_embeddings[name] = tool_embedding\n",
    "        print(\"Added tools with embeddings:\", list(self.tool_embeddings.keys()))\n",
    "        \n",
    "    def retrieve(self, query: str) -> list[Callable[..., str]]:\n",
    "        query_embedding = self.embedding_model.embed_texts([query])[0]\n",
    "\n",
    "        similarities: list[tuple[float, str]] = []\n",
    "        for tool_name, tool_embedding in self.tool_embeddings.items():\n",
    "            similarity = cosine_similarity(query_embedding, tool_embedding)\n",
    "            similarities.append((similarity, tool_name))\n",
    "\n",
    "        similarities.sort(reverse=True)\n",
    "\n",
    "        print(\"Tool similarities:\", similarities)\n",
    "\n",
    "        elbow_threshold = elbow_method([sim for sim, _ in similarities])\n",
    "        final_threshold = max(self.threshold, elbow_threshold)\n",
    "\n",
    "        top_tools = [self.tool_handler.tool_mappings[tool_name] for similarity, tool_name in similarities if similarity >= final_threshold]\n",
    "\n",
    "        print(\"Retrieved tools:\", [tool.__name__ for tool in top_tools])\n",
    "        \n",
    "        return top_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c8ca7ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InMemoryConversationRepository(ConversationRepository):\n",
    "    def __init__(self) -> None:\n",
    "        self.conversations: dict[str, list[MessageLike]] = {}\n",
    "\n",
    "    def get_conversation_history(self, conversation_id: str) -> list[MessageLike]:\n",
    "        return self.conversations.get(conversation_id, [])\n",
    "\n",
    "    def add_messages(self, conversation_id: str, messages: list[MessageLike]) -> None:\n",
    "        if conversation_id not in self.conversations:\n",
    "            self.conversations[conversation_id] = []\n",
    "        self.conversations[conversation_id].extend(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "13744318",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ToolKit:\n",
    "    def registry(self) -> list[Callable[..., str]]:\n",
    "        \"\"\"Return a list of all available tools.\"\"\"\n",
    "        return [\n",
    "            self.get_weather,\n",
    "            self.get_user_profile,\n",
    "            self.create_reminder,\n",
    "            self.get_news,\n",
    "            self.code_interpreter,\n",
    "            self.get_joke,\n",
    "            self.add,\n",
    "            self.subtract,\n",
    "            self.multiply,\n",
    "            self.divide,\n",
    "        ]\n",
    "    \n",
    "    def get_weather(self, location: str) -> str:\n",
    "        \"\"\"Get the current weather for a given location.\"\"\"\n",
    "        # Dummy implementation for illustration\n",
    "        return f\"The current weather in {location} is sunny with a temperature of 25Â°C.\"\n",
    "\n",
    "    def get_user_profile(self, user_id: str) -> str:\n",
    "        \"\"\"Get the user profile information for a given user ID.\"\"\"\n",
    "        # Dummy implementation for illustration\n",
    "        return f\"User {user_id} is a 30-year-old software developer from San Francisco.\"\n",
    "\n",
    "    def create_reminder(self, user_id: str, reminder_text: str, time: str) -> str:\n",
    "        \"\"\"Create a reminder for the user.\"\"\"\n",
    "        # Dummy implementation for illustration\n",
    "        return f\"Reminder for user {user_id}: '{reminder_text}' at {time} has been created.\"\n",
    "\n",
    "    def get_news(self, topic: str) -> str:\n",
    "        \"\"\"Get the latest news on a given topic.\"\"\"\n",
    "        # Dummy implementation for illustration\n",
    "        return f\"The latest news on {topic} is that everything is going great!\"\n",
    "\n",
    "    def code_interpreter(self, code: str) -> str:\n",
    "        \"\"\"Execute the given code and return the output.\"\"\"\n",
    "        try:\n",
    "            # WARNING: Using eval/exec can be dangerous. This is just for illustration.\n",
    "            local_vars = {}\n",
    "            exec(code, {}, local_vars)\n",
    "            return str(local_vars.get('result', 'No result variable defined.'))\n",
    "        except Exception as e:\n",
    "            return f\"Error executing code: {e}\"\n",
    "        \n",
    "    def get_joke(self) -> str:\n",
    "        \"\"\"Get a random joke.\"\"\"\n",
    "        # Dummy implementation for illustration\n",
    "        return \"Why did the scarecrow win an award? Because he was outstanding in his field!\"\n",
    "\n",
    "    def add(self, a: float, b: float) -> str:\n",
    "        \"\"\"Add two numbers.\"\"\"\n",
    "        return str(a + b)\n",
    "\n",
    "    def subtract(self, a: float, b: float) -> str:\n",
    "        \"\"\"Subtract two numbers.\"\"\"\n",
    "        return str(a - b)\n",
    "\n",
    "    def multiply(self, a: float, b: float) -> str:\n",
    "        \"\"\"Multiply two numbers.\"\"\"\n",
    "        return str(a * b)\n",
    "\n",
    "    def divide(self, a: float, b: float) -> str:\n",
    "        \"\"\"Divide two numbers.\"\"\"\n",
    "        if b == 0:\n",
    "            return \"Error: Division by zero.\"\n",
    "        return str(a / b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b0847824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings given model text-embedding-3-small for texts: ['get_weather: Get the current weather for a given location.', 'get_user_profile: Get the user profile information for a given user ID.', 'create_reminder: Create a reminder for the user.', 'get_news: Get the latest news on a given topic.', 'code_interpreter: Execute the given code and return the output.', 'get_joke: Get a random joke.', 'add: Add two numbers.', 'subtract: Subtract two numbers.', 'multiply: Multiply two numbers.', 'divide: Divide two numbers.']\n",
      "Added tools with embeddings: ['get_weather', 'get_user_profile', 'create_reminder', 'get_news', 'code_interpreter', 'get_joke', 'add', 'subtract', 'multiply', 'divide']\n"
     ]
    }
   ],
   "source": [
    "embedding_model = OpenAIEmbeddingModel(model_name=\"text-embedding-3-small\")\n",
    "\n",
    "tool_kit = ToolKit()\n",
    "\n",
    "tool_handler = ToolHandler(tools=tool_kit.registry())\n",
    "\n",
    "tool_router = EmbeddingBasedToolRouter(tool_handler=tool_handler, embedding_model=embedding_model)\n",
    "\n",
    "llm = OpenAILLM(model_name=\"gpt-4.1-mini\")\n",
    "\n",
    "agent = Agent(llm=llm, tool_handler=tool_handler)\n",
    "\n",
    "conversation_repository = InMemoryConversationRepository()\n",
    "\n",
    "context_middleware = ContextMiddleware(\n",
    "    conversation_repository=conversation_repository,\n",
    "    tool_router=tool_router\n",
    ")\n",
    "\n",
    "agent_service = AgentService(\n",
    "    agent=agent,\n",
    "    context_middleware=context_middleware\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "09fd577f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings given model text-embedding-3-small for texts: [\"What's 13 + 24 * 2 - 5 / (1 + 1)?\"]\n",
      "Tool similarities: [(0.41344234278303993, 'multiply'), (0.3905658599540017, 'subtract'), (0.37787729461485337, 'add'), (0.37557557448783335, 'divide'), (0.19657426433152653, 'code_interpreter'), (0.14942267334605042, 'get_joke'), (0.11217535548130472, 'create_reminder'), (0.05213801635570409, 'get_weather'), (0.04245934768334578, 'get_user_profile'), (0.03898759808035919, 'get_news')]\n",
      "Retrieved tools: ['multiply', 'subtract', 'add', 'divide']\n",
      "Agent running with messages: [{'content': \"What's 13 + 24 * 2 - 5 / (1 + 1)?\", 'role': 'user'}]\n",
      "Generating text with model gpt-4.1-mini and tools: ['multiply', 'subtract', 'add', 'divide']\n",
      "\n",
      "[First token received in 3.45 seconds]\n",
      "\n",
      "\n",
      "[Action] Calling tool multiply with arguments {'a': 24, 'b': 2}\n",
      "\n",
      "\n",
      "[Action] Calling tool add with arguments {'a': 1, 'b': 1}\n",
      "\n",
      "Agent executing tool: multiply with arguments {'a': 24, 'b': 2}\n",
      "\n",
      "[Action Result] 48\n",
      "\n",
      "Agent executing tool: add with arguments {'a': 1, 'b': 1}\n",
      "\n",
      "[Action Result] 2\n",
      "\n",
      "Agent running with messages: [{'content': \"What's 13 + 24 * 2 - 5 / (1 + 1)?\", 'role': 'user'}, {'role': 'assistant', 'tool_calls': [{'id': 'call_fIsAw1cj743nM6X7LDZwIBY0', 'type': 'function', 'function': {'name': 'multiply', 'arguments': '{\"a\": 24, \"b\": 2}'}}]}, {'role': 'tool', 'tool_call_id': 'call_fIsAw1cj743nM6X7LDZwIBY0', 'content': '48'}, {'role': 'assistant', 'tool_calls': [{'id': 'call_FLuelRP4Z2nDuNiRDlVEGVLK', 'type': 'function', 'function': {'name': 'add', 'arguments': '{\"a\": 1, \"b\": 1}'}}]}, {'role': 'tool', 'tool_call_id': 'call_FLuelRP4Z2nDuNiRDlVEGVLK', 'content': '2'}]\n",
      "Generating text with model gpt-4.1-mini and tools: ['multiply', 'subtract', 'add', 'divide']\n",
      "\n",
      "[Action] Calling tool divide with arguments {'a': 5, 'b': 2}\n",
      "\n",
      "Agent executing tool: divide with arguments {'a': 5, 'b': 2}\n",
      "\n",
      "[Action Result] 2.5\n",
      "\n",
      "Agent running with messages: [{'content': \"What's 13 + 24 * 2 - 5 / (1 + 1)?\", 'role': 'user'}, {'role': 'assistant', 'tool_calls': [{'id': 'call_fIsAw1cj743nM6X7LDZwIBY0', 'type': 'function', 'function': {'name': 'multiply', 'arguments': '{\"a\": 24, \"b\": 2}'}}]}, {'role': 'tool', 'tool_call_id': 'call_fIsAw1cj743nM6X7LDZwIBY0', 'content': '48'}, {'role': 'assistant', 'tool_calls': [{'id': 'call_FLuelRP4Z2nDuNiRDlVEGVLK', 'type': 'function', 'function': {'name': 'add', 'arguments': '{\"a\": 1, \"b\": 1}'}}]}, {'role': 'tool', 'tool_call_id': 'call_FLuelRP4Z2nDuNiRDlVEGVLK', 'content': '2'}, {'role': 'assistant', 'tool_calls': [{'id': 'call_jOUE3vmmytHwJDw1WhUqqsuI', 'type': 'function', 'function': {'name': 'divide', 'arguments': '{\"a\": 5, \"b\": 2}'}}]}, {'role': 'tool', 'tool_call_id': 'call_jOUE3vmmytHwJDw1WhUqqsuI', 'content': '2.5'}]\n",
      "Generating text with model gpt-4.1-mini and tools: ['multiply', 'subtract', 'add', 'divide']\n",
      "\n",
      "[Action] Calling tool add with arguments {'a': 13, 'b': 48}\n",
      "\n",
      "\n",
      "[Action] Calling tool subtract with arguments {'a': 61, 'b': 2.5}\n",
      "\n",
      "Agent executing tool: add with arguments {'a': 13, 'b': 48}\n",
      "\n",
      "[Action Result] 61\n",
      "\n",
      "Agent executing tool: subtract with arguments {'a': 61, 'b': 2.5}\n",
      "\n",
      "[Action Result] 58.5\n",
      "\n",
      "Agent running with messages: [{'content': \"What's 13 + 24 * 2 - 5 / (1 + 1)?\", 'role': 'user'}, {'role': 'assistant', 'tool_calls': [{'id': 'call_fIsAw1cj743nM6X7LDZwIBY0', 'type': 'function', 'function': {'name': 'multiply', 'arguments': '{\"a\": 24, \"b\": 2}'}}]}, {'role': 'tool', 'tool_call_id': 'call_fIsAw1cj743nM6X7LDZwIBY0', 'content': '48'}, {'role': 'assistant', 'tool_calls': [{'id': 'call_FLuelRP4Z2nDuNiRDlVEGVLK', 'type': 'function', 'function': {'name': 'add', 'arguments': '{\"a\": 1, \"b\": 1}'}}]}, {'role': 'tool', 'tool_call_id': 'call_FLuelRP4Z2nDuNiRDlVEGVLK', 'content': '2'}, {'role': 'assistant', 'tool_calls': [{'id': 'call_jOUE3vmmytHwJDw1WhUqqsuI', 'type': 'function', 'function': {'name': 'divide', 'arguments': '{\"a\": 5, \"b\": 2}'}}]}, {'role': 'tool', 'tool_call_id': 'call_jOUE3vmmytHwJDw1WhUqqsuI', 'content': '2.5'}, {'role': 'assistant', 'tool_calls': [{'id': 'call_RaxJeeW9MC5mqd22qoHAjjJM', 'type': 'function', 'function': {'name': 'add', 'arguments': '{\"a\": 13, \"b\": 48}'}}]}, {'role': 'tool', 'tool_call_id': 'call_RaxJeeW9MC5mqd22qoHAjjJM', 'content': '61'}, {'role': 'assistant', 'tool_calls': [{'id': 'call_inqmiYgvWEgEq7prBSTumcWY', 'type': 'function', 'function': {'name': 'subtract', 'arguments': '{\"a\": 61, \"b\": 2.5}'}}]}, {'role': 'tool', 'tool_call_id': 'call_inqmiYgvWEgEq7prBSTumcWY', 'content': '58.5'}]\n",
      "Generating text with model gpt-4.1-mini and tools: ['multiply', 'subtract', 'add', 'divide']\n",
      "The result of the expression 13 + 24 * 2 - 5 / (1 + 1) is 58.5.\n",
      "[Completed]\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "gen = agent_service.handle_request(\n",
    "    conversation_id=\"conv1\",\n",
    "    query=\"What's 13 + 24 * 2 - 5 / (1 + 1)?\"\n",
    ")\n",
    "\n",
    "first_token = True\n",
    "for event in gen:\n",
    "    if first_token:\n",
    "        elapsed = timeit.default_timer() - start_time\n",
    "        print(f\"\\n[First token received in {elapsed:.2f} seconds]\\n\")\n",
    "        first_token = False\n",
    "        \n",
    "    if event.event == AgentStreamingEventType.TOKEN:\n",
    "        print(event.data, end=\"\", flush=True)\n",
    "    elif event.event == AgentStreamingEventType.ACTION and event.action_id:\n",
    "        print(f\"\\n[Action] {event.data}\\n\")\n",
    "    elif event.event == AgentStreamingEventType.ACTION_RESULT:\n",
    "        print(f\"\\n[Action Result] {event.data}\\n\")\n",
    "    elif event.event == AgentStreamingEventType.COMPLETED:\n",
    "        print(\"\\n[Completed]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e002e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dynamic-tool-router (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
