{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6180aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from functools import update_wrapper\n",
    "from typing import Callable, Any, Optional, Dict, TypeVar, overload, Union, cast, Literal\n",
    "from pydantic import create_model\n",
    "from openai.types.responses.tool_param import ToolParam\n",
    "from openai.types.chat import ChatCompletionToolUnionParam\n",
    "import inspect\n",
    "from enum import Enum\n",
    "\n",
    "class OpenAIAPIProtocol(str, Enum):\n",
    "    CHAT_COMPLETIONS = \"chat_completions\"\n",
    "    RESPONSES = \"responses\"\n",
    "\n",
    "@overload\n",
    "def build_tool_schema(\n",
    "    func: Callable[..., Any], \n",
    "    name: str,\n",
    "    api: Literal[OpenAIAPIProtocol.RESPONSES],\n",
    "    description: str = \"\",\n",
    ") -> ToolParam: ... # type: ignore\n",
    "\n",
    "@overload\n",
    "def build_tool_schema(\n",
    "    func: Callable[..., Any], \n",
    "    name: str,\n",
    "    api: Literal[OpenAIAPIProtocol.CHAT_COMPLETIONS],\n",
    "    description: str = \"\",\n",
    ") -> ChatCompletionToolUnionParam: ... # type: ignore\n",
    "\n",
    "def build_tool_schema(\n",
    "    func: Callable[..., Any], \n",
    "    name: str,\n",
    "    api: OpenAIAPIProtocol,\n",
    "    description: str = \"\",\n",
    ") -> Union[ToolParam, ChatCompletionToolUnionParam]:\n",
    "    signature = inspect.signature(func)\n",
    "    final_description = inspect.getdoc(func) or description\n",
    "\n",
    "    model_fields = {\n",
    "        name: (param.annotation, ...)\n",
    "        for name, param in signature.parameters.items()\n",
    "    }\n",
    "\n",
    "    ToolArguments = create_model(\"ToolArguments\", **model_fields) # type: ignore\n",
    "    raw_schema = ToolArguments.model_json_schema() # type: ignore\n",
    "    raw_schema = cast(Dict[str, Any], raw_schema)\n",
    "    raw_schema.pop(\"title\", None) # type: ignore\n",
    "    raw_schema[\"additionalProperties\"] = False\n",
    "\n",
    "    if api == OpenAIAPIProtocol.RESPONSES:\n",
    "        schema: dict[str, Any] = {\n",
    "            \"type\": \"function\",\n",
    "            \"name\": name,\n",
    "            \"description\": final_description,\n",
    "            \"parameters\": raw_schema,\n",
    "            \"strict\": True,\n",
    "        }\n",
    "        return cast(ToolParam, schema)\n",
    "    \n",
    "    else:\n",
    "        schema: dict[str, Any] = {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": name,\n",
    "                \"description\": final_description,\n",
    "                \"strict\": True,\n",
    "                \"parameters\": raw_schema,\n",
    "            },\n",
    "        }\n",
    "\n",
    "        return cast(ChatCompletionToolUnionParam, schema)\n",
    "\n",
    "class Tool:\n",
    "    \"\"\"\n",
    "    Wrapper that makes a function into a tool with a schema.\n",
    "    \"\"\"\n",
    "    __tool_wrapped__ = True\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        func: Optional[Callable[..., Any]] = None,\n",
    "        *,\n",
    "        schema: Optional[Union[ToolParam, ChatCompletionToolUnionParam]] = None,\n",
    "        api: OpenAIAPIProtocol = OpenAIAPIProtocol.CHAT_COMPLETIONS,\n",
    "    ) -> None:\n",
    "        if func is None:\n",
    "            func = self.__call__\n",
    "\n",
    "        self._func = func\n",
    "\n",
    "        if schema is None:\n",
    "            schema = build_tool_schema(self._func, name=str(self.__class__.__name__), api=api)\n",
    "        \n",
    "        self.schema = schema\n",
    "        update_wrapper(self, self._func)\n",
    "\n",
    "    @classmethod\n",
    "    def from_decorator(\n",
    "        cls,\n",
    "        func: Callable[..., Any],\n",
    "        *,\n",
    "        schema: Union[ToolParam, ChatCompletionToolUnionParam],\n",
    "    ) -> \"Tool\":\n",
    "        \"\"\" \n",
    "        Create a Tool instance from a function and a schema.\n",
    "        This is used internally by the `tool` decorator.\n",
    "        \"\"\"\n",
    "        tool_instance = cls(func, schema=schema)\n",
    "        return tool_instance\n",
    "\n",
    "    def __call__(self, *args: Any, **kwargs: Any) -> Any:\n",
    "        if inspect.iscoroutinefunction(self._func):\n",
    "            return self._func(*args, **kwargs)\n",
    "        return self._func(*args, **kwargs)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"<Tool {self._func.__name__}>\"\n",
    "    \n",
    "    def __name__(self) -> str:\n",
    "        return self._func.__name__\n",
    "\n",
    "T = TypeVar(\"T\", bound=Callable[..., Any])\n",
    "\n",
    "@overload\n",
    "def tool(\n",
    "    func: Callable[..., Any],\n",
    "    *,\n",
    "    description: str = \"\",\n",
    "    api: OpenAIAPIProtocol = OpenAIAPIProtocol.CHAT_COMPLETIONS,\n",
    ") -> Tool: ... # type: ignore\n",
    "\n",
    "@overload\n",
    "def tool(\n",
    "    *,\n",
    "    description: str = \"\",\n",
    "    api: OpenAIAPIProtocol = OpenAIAPIProtocol.CHAT_COMPLETIONS,\n",
    ") -> Callable[[Callable[..., Any]], Tool]: ... # type: ignore\n",
    "\n",
    "def tool(\n",
    "    func: Optional[T] = None,\n",
    "    *,\n",
    "    description: str = \"\",\n",
    "    api: OpenAIAPIProtocol = OpenAIAPIProtocol.CHAT_COMPLETIONS,\n",
    ") -> Union[Tool, Callable[[T], Tool]]:\n",
    "    \"\"\"\n",
    "    Decorator to wrap a function into a Tool with OpenAI function-calling schema.\n",
    "    \"\"\"\n",
    "    def decorator(inner_func: T) -> Tool:\n",
    "        schema = build_tool_schema(inner_func, name=inner_func.__name__, api=api, description=description)\n",
    "\n",
    "        return Tool.from_decorator(inner_func, schema=schema)\n",
    "\n",
    "    # If used without args: @tool\n",
    "    return decorator if func is None else decorator(func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cb0582",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import Generator, Callable, Any, Generic, TypedDict, Required\n",
    "from pydantic.dataclasses import dataclass\n",
    "from openai.types.responses import ResponseInputItemParam, ResponseFunctionToolCallParam\n",
    "from openai.types.responses.response_output_message_param import ResponseOutputMessageParam\n",
    "from openai.types.responses.response_reasoning_item_param import ResponseReasoningItemParam, Summary\n",
    "from openai.types.responses.response_input_item_param import FunctionCallOutput\n",
    "from enum import Enum\n",
    "import json\n",
    "\n",
    "MessageLike = ResponseInputItemParam\n",
    "\n",
    "class StreamingEventType(Enum):\n",
    "    REASONING_TOKEN = \"reasoning_token\"\n",
    "    REASONING = \"reasoning\"\n",
    "    TOKEN = \"token\"\n",
    "    TOOL_CALL = \"tool_call\"\n",
    "    TOOL_RESULT = \"tool_result\"\n",
    "    COMPLETED = \"completed\"\n",
    "\n",
    "class AgentStreamingEventType(Enum):\n",
    "    REASONING = \"reasoning\"\n",
    "    TOKEN = \"token\"\n",
    "    ACTION = \"action\"\n",
    "    ACTION_RESULT = \"action_result\"\n",
    "    COMPLETED = \"completed\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ToolCall:\n",
    "    name: str\n",
    "    tool_call_id: str\n",
    "    arguments: dict[str, Any]\n",
    "\n",
    "@dataclass\n",
    "class Context:\n",
    "    conversation_id: str\n",
    "    user_query: str\n",
    "    messages: list[MessageLike]\n",
    "    tools: set[Callable[..., str]]\n",
    "\n",
    "@dataclass\n",
    "class LLMStreamingEvent:\n",
    "    event: StreamingEventType\n",
    "    data: str\n",
    "    tool_call_id: str | None = None\n",
    "    tool_calls: list[ToolCall] | None = None\n",
    "\n",
    "@dataclass\n",
    "class AgentStreamingEvent:\n",
    "    event: AgentStreamingEventType\n",
    "    data: str\n",
    "    action_id: str | None = None\n",
    "\n",
    "class LLM(ABC):\n",
    "    @abstractmethod\n",
    "    def generate_text(self, messages: list[MessageLike], tools: Optional[set[Callable[..., str]]] = None) -> Generator[LLMStreamingEvent, None, None]:\n",
    "        \"\"\"Generate text based on the given prompt.\"\"\"\n",
    "        pass\n",
    "\n",
    "class EmbeddingModel(ABC):\n",
    "    @abstractmethod\n",
    "    def embed_texts(self, texts: list[str]) -> list[list[float]]:\n",
    "        \"\"\"Generate embedding for the given texts.\"\"\"\n",
    "        pass\n",
    "\n",
    "TIn = TypeVar(\"TIn\")\n",
    "TOut = TypeVar(\"TOut\")\n",
    "TNext = TypeVar(\"TNext\")\n",
    "\n",
    "class Handler(ABC, Generic[TIn, TOut]):\n",
    "    @abstractmethod\n",
    "    def process(self, input: TIn) -> TOut:\n",
    "        pass\n",
    "\n",
    "    def handle(self, input: TIn) -> TOut:\n",
    "        return self.process(input)\n",
    "\n",
    "    def then(self, next_handler: Handler[TOut, TNext]) -> Chain[TIn, TNext]:\n",
    "        return Chain(self, next_handler)\n",
    "\n",
    "class Chain(Handler[TIn, TOut]):\n",
    "    \"\"\"Represents two linked handlers as a single unit.\"\"\"\n",
    "    def __init__(self, first: Handler[TIn, TNext], second: Handler[TNext, TOut]):\n",
    "        self.first = first\n",
    "        self.second = second\n",
    "\n",
    "    def process(self, input: TIn) -> TOut:\n",
    "        intermediate = self.first.handle(input)\n",
    "        return self.second.handle(intermediate)\n",
    "\n",
    "T = TypeVar(\"T\")\n",
    "\n",
    "class Command(ABC, Generic[T]):\n",
    "    @abstractmethod\n",
    "    def exec(self, input: T) -> None:\n",
    "        pass\n",
    "\n",
    "class ToolHandler(Handler[ToolCall, str]):\n",
    "    def __init__(\n",
    "        self, \n",
    "        tools: set[Callable[..., str]],\n",
    "        on_error: Optional[Callable[[ToolCall, Exception], str]] = None,\n",
    "    ) -> None:\n",
    "        self.tool_mappings: dict[str, Callable[..., str]] = {}\n",
    "        self.add_tools(tools)\n",
    "        self.on_error = on_error\n",
    "\n",
    "    @property\n",
    "    def tools(self) -> set[Callable[..., str]]:\n",
    "        return set(self.tool_mappings.values())\n",
    "    \n",
    "    @property\n",
    "    def tool_names(self) -> set[str]:\n",
    "        return set(self.tool_mappings.keys())\n",
    "    \n",
    "    def clear_tools(self) -> None:\n",
    "        \"\"\"Remove all tools from the handler.\"\"\"\n",
    "        self.tool_mappings = {}\n",
    "    \n",
    "    def get_tool(self, name: str) -> Optional[Callable[..., str]]:\n",
    "        return self.tool_mappings.get(name)\n",
    "        \n",
    "    def add_tools(self, tools: set[Callable[..., str]]) -> None:\n",
    "        \"\"\"Add tools to the handler.\"\"\"\n",
    "        for tool in tools:\n",
    "            if tool.__name__ in self.tool_mappings:\n",
    "                continue\n",
    "            self.tool_mappings[tool.__name__] = tool\n",
    "\n",
    "    def set_tools(self, tools: set[Callable[..., str]]) -> None:\n",
    "        \"\"\"Set the tools for the handler, replacing any existing tools.\"\"\"\n",
    "        self.clear_tools()\n",
    "        self.add_tools(tools)\n",
    "\n",
    "    def update_tool(self, tool: Callable[..., str]) -> None:\n",
    "        \"\"\"Update or add a single tool in the handler.\"\"\"\n",
    "        self.tool_mappings[tool.__name__] = tool\n",
    "\n",
    "    def remove_tool(self, name: str) -> None:\n",
    "        \"\"\"Remove a tool by name from the handler.\"\"\"\n",
    "        if name in self.tool_mappings:\n",
    "            del self.tool_mappings[name]\n",
    "\n",
    "    def process(self, input: ToolCall) -> str:\n",
    "        return self.execute_tool(input)\n",
    "\n",
    "    def execute_tool(self, tool_call: ToolCall) -> str:\n",
    "        tool = self.tool_mappings.get(tool_call.name)\n",
    "        if not tool:\n",
    "            return f\"Error: Tool '{tool_call.name}' not found.\"\n",
    "\n",
    "        try:\n",
    "            return tool(**tool_call.arguments)\n",
    "        except Exception as e:\n",
    "            if self.on_error:\n",
    "                return self.on_error(tool_call, e)\n",
    "            else:\n",
    "                return f\"Error executing tool '{tool_call.name}': {str(e)}\"\n",
    "    \n",
    "class ToolRouter(ToolHandler):\n",
    "    \"\"\"Base class for tool routing based on queries.\"\"\"\n",
    "    @abstractmethod\n",
    "    def retrieve(self, query: str) -> set[Callable[..., str]]:\n",
    "        \"\"\"Retrieve appropriate tools based on the query.\"\"\"\n",
    "        pass\n",
    "\n",
    "ToolUseBehavior = Literal[\n",
    "    \"stop_on_tool_call\", \n",
    "    \"stop_on_tool_result\", \n",
    "    \"auto\"\n",
    "]\n",
    "\n",
    "ToolName = str\n",
    "\n",
    "class Agent:\n",
    "    def __init__(\n",
    "        self, \n",
    "        llm: LLM, \n",
    "        tool_handler: ToolHandler,\n",
    "        tool_use_behavior: Optional[ToolUseBehavior] = \"auto\",\n",
    "        on_tool_call: Optional[dict[ToolName, Command[ToolCall]]] = None,\n",
    "        on_tool_result: Optional[dict[ToolName, Command[tuple[ToolCall, str]]]] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.llm = llm\n",
    "        self.tool_handler = tool_handler\n",
    "        self.tool_use_behavior = tool_use_behavior\n",
    "\n",
    "        self.on_tool_call = on_tool_call\n",
    "        self.on_tool_result = on_tool_result\n",
    "\n",
    "    def add_tools(self, tools: set[Callable[..., str]]) -> None:\n",
    "        \"\"\"Add tools to the agent.\"\"\"\n",
    "        self.tool_handler.add_tools(tools)\n",
    "\n",
    "    def run(self, messages: list[MessageLike]) -> Generator[LLMStreamingEvent, None, None]:\n",
    "        \"\"\"Run the agent with the given prompt.\"\"\"\n",
    "        while True:\n",
    "            for delta in self.llm.generate_text(messages, tools=self.tool_handler.tools):\n",
    "                if delta.event == StreamingEventType.REASONING_TOKEN:\n",
    "                    yield delta\n",
    "                if delta.event == StreamingEventType.REASONING:\n",
    "                    yield delta\n",
    "                    messages.append( \n",
    "                        ResponseReasoningItemParam( # type: ignore\n",
    "                            type=\"reasoning\",\n",
    "                            summary=[Summary(\n",
    "                                type=\"summary_text\",\n",
    "                                text=delta.data\n",
    "                            )],\n",
    "                        )\n",
    "                    )\n",
    "                elif delta.event == StreamingEventType.TOOL_CALL and delta.tool_calls:\n",
    "                    yield delta\n",
    "                    \n",
    "                    for tool_call in delta.tool_calls:\n",
    "                        messages.append(\n",
    "                            ResponseFunctionToolCallParam(\n",
    "                                type=\"function_call\",\n",
    "                                name=tool_call.name,\n",
    "                                arguments=json.dumps(tool_call.arguments),\n",
    "                                call_id=tool_call.tool_call_id,\n",
    "                            )\n",
    "                        )\n",
    "\n",
    "                        if self.on_tool_call:\n",
    "                            command = self.on_tool_call.get(tool_call.name)\n",
    "                            if command:\n",
    "                                command.exec(tool_call)\n",
    "                    \n",
    "                    if self.tool_use_behavior == \"stop_before\":\n",
    "                        return\n",
    "                    \n",
    "                    for tool_call in delta.tool_calls:\n",
    "                        tool_result = self.tool_handler.execute_tool(tool_call)\n",
    "                        yield LLMStreamingEvent(\n",
    "                            event=StreamingEventType.TOOL_RESULT,\n",
    "                            tool_call_id=tool_call.tool_call_id,\n",
    "                            data=tool_result,\n",
    "                        )\n",
    "                        messages.append(\n",
    "                            FunctionCallOutput(\n",
    "                                type=\"function_call_output\",\n",
    "                                output=tool_result,\n",
    "                                call_id=tool_call.tool_call_id,\n",
    "                            )\n",
    "                        )\n",
    "\n",
    "                        if self.on_tool_result:\n",
    "                            command = self.on_tool_result.get(tool_call.name)\n",
    "                            if command:\n",
    "                                command.exec((tool_call, tool_result))\n",
    "                        \n",
    "                    if self.tool_use_behavior == \"stop_after\":\n",
    "                        return\n",
    "                elif delta.event == StreamingEventType.COMPLETED:\n",
    "                    yield LLMStreamingEvent(\n",
    "                        event=StreamingEventType.COMPLETED,\n",
    "                        data=delta.data,\n",
    "                    )\n",
    "                    messages.append(\n",
    "                        ResponseOutputMessageParam( # type: ignore\n",
    "                            role=\"assistant\",\n",
    "                            content=delta.data\n",
    "                        )\n",
    "                    )\n",
    "                    return\n",
    "                elif delta.event == StreamingEventType.TOKEN:\n",
    "                    yield LLMStreamingEvent(\n",
    "                        event=StreamingEventType.TOKEN,\n",
    "                        data=delta.data,\n",
    "                    )\n",
    "                \n",
    "class ConversationRepository(ABC):\n",
    "    @abstractmethod\n",
    "    def get_conversation_history(self, conversation_id: str) -> list[MessageLike]:\n",
    "        \"\"\"Retrieve the conversation history for the given conversation ID.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def add_messages(self, conversation_id: str, messages: list[MessageLike]) -> None:\n",
    "        \"\"\"Add messages to the conversation history.\"\"\"\n",
    "        pass\n",
    "\n",
    "class ContextPrepareParam(TypedDict):\n",
    "    conversation_id: Required[str]\n",
    "    query: Required[str]\n",
    "\n",
    "class ContextMiddleware(Handler[ContextPrepareParam, Context]):\n",
    "    def __init__(\n",
    "        self, \n",
    "        conversation_repository: ConversationRepository,\n",
    "        tool_router: ToolRouter\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.conversation_repository = conversation_repository\n",
    "        self.tool_router = tool_router\n",
    "        \n",
    "    def process(self, input: ContextPrepareParam) -> Context:\n",
    "        \"\"\"Prepare context for the LLM based on conversation ID and query.\"\"\"\n",
    "        history = self.conversation_repository.get_conversation_history(input[\"conversation_id\"])\n",
    "        tools = self.tool_router.retrieve(input[\"query\"])\n",
    "\n",
    "        history.append({\"role\": \"user\", \"content\": input[\"query\"]})\n",
    "        \n",
    "        return Context(\n",
    "            conversation_id=input[\"conversation_id\"],\n",
    "            user_query=input[\"query\"],\n",
    "            messages=history,\n",
    "            tools=tools\n",
    "        )\n",
    "\n",
    "class AgentService:\n",
    "    def __init__(\n",
    "        self, \n",
    "        agent: Agent,\n",
    "        context_middleware: ContextMiddleware\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.agent = agent\n",
    "        self.context_middleware = context_middleware\n",
    "\n",
    "    def handle_request(self, conversation_id: str, query: str) -> Generator[AgentStreamingEvent, None, None]:\n",
    "        \"\"\"Handle the request by preparing context and generating text.\"\"\"\n",
    "        context = self.context_middleware.process({\"conversation_id\": conversation_id, \"query\": query})\n",
    "        self.agent.add_tools(context.tools)\n",
    "        for delta in self.agent.run(context.messages):\n",
    "            if delta.event == StreamingEventType.TOOL_CALL and delta.tool_calls:\n",
    "                for tool_call in delta.tool_calls:\n",
    "                    yield AgentStreamingEvent(\n",
    "                        event=AgentStreamingEventType.ACTION,\n",
    "                        data=f\"Calling tool {tool_call.name} with arguments {tool_call.arguments}\",\n",
    "                        action_id=tool_call.tool_call_id or \"\"\n",
    "                    )\n",
    "            \n",
    "            elif delta.event == StreamingEventType.TOOL_RESULT:\n",
    "                yield AgentStreamingEvent(\n",
    "                    event=AgentStreamingEventType.ACTION_RESULT,\n",
    "                    data=delta.data,\n",
    "                    action_id=delta.tool_call_id or \"\"\n",
    "                )\n",
    "\n",
    "            elif delta.event == StreamingEventType.COMPLETED:\n",
    "                self.context_middleware.conversation_repository.add_messages(\n",
    "                    conversation_id,\n",
    "                    [{\"role\": \"user\", \"content\": query}, {\"role\": \"assistant\", \"content\": delta.data}]\n",
    "                )\n",
    "                yield AgentStreamingEvent(\n",
    "                    event=AgentStreamingEventType.COMPLETED,\n",
    "                    data=delta.data\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                yield AgentStreamingEvent(\n",
    "                    event=AgentStreamingEventType.TOKEN,\n",
    "                    data=delta.data\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a534d862",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StringToIntHandler(Handler[str, int]):\n",
    "    def process(self, input: str) -> int:\n",
    "        return len(input)\n",
    "\n",
    "class IntToFloatHandler(Handler[int, float]):\n",
    "    def process(self, input: int) -> float:\n",
    "        return input / 2\n",
    "\n",
    "class FloatToStringHandler(Handler[float, str]):\n",
    "    def process(self, input: float) -> str:\n",
    "        return f\"Result is {input}\"\n",
    "\n",
    "# Chain them\n",
    "chain = StringToIntHandler()\\\n",
    "        .then(IntToFloatHandler())\\\n",
    "        .then(FloatToStringHandler())\\\n",
    "\n",
    "result = chain.handle(\"hello\")\n",
    "print(result)  # Result is 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa8e7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from openai.types.responses import ResponseStreamEvent\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# generator = client.responses.create(\n",
    "#     model=\"gpt-5.1\",\n",
    "#     input=\"What's the difference between MCP Servers and traditional function calling?\",\n",
    "#     reasoning={\n",
    "#         \"effort\": \"medium\",\n",
    "#         \"summary\": \"auto\"\n",
    "#     },\n",
    "#     stream=True\n",
    "# )\n",
    "\n",
    "# events: list[ResponseStreamEvent] = []\n",
    "# for event in generator:\n",
    "#     events.append(event)\n",
    "#     print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b088a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.types.responses import (\n",
    "    ResponseTextDeltaEvent, \n",
    "    ResponseReasoningSummaryTextDeltaEvent,\n",
    "    ResponseCompletedEvent,\n",
    "    ResponseFunctionToolCall,\n",
    "    ResponseReasoningSummaryTextDoneEvent\n",
    ")\n",
    "from openai.types.chat_model import ChatModel\n",
    "from openai.types.shared_params import Reasoning\n",
    "from typing import Sequence\n",
    "import re\n",
    "\n",
    "\n",
    "class OpenAILLMResponsesAPI(LLM):\n",
    "    def __init__(\n",
    "        self, \n",
    "        client: OpenAI, \n",
    "        model: ChatModel = \"gpt-4o-mini\",\n",
    "        allow_parallel_tool_calls: bool = True,\n",
    "        temperature: float = 0.7,\n",
    "        top_p: float = 1.0,\n",
    "        reasoning: Optional[Reasoning] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.client = client\n",
    "        self.model = model\n",
    "        self.allow_parallel_tool_calls = allow_parallel_tool_calls\n",
    "        self.temperature = temperature\n",
    "        self.top_p = top_p\n",
    "        self.reasoning = reasoning\n",
    "\n",
    "        # If reasoning is enabled, disable temperature and top_p as they are not supported\n",
    "        if self._is_reasoning_model():\n",
    "            self.temperature = None\n",
    "            self.top_p = None\n",
    "        else:\n",
    "            self.reasoning = None\n",
    "    \n",
    "    def _is_reasoning_model(self) -> bool:\n",
    "        pattern = r\"(gpt-5\\S*|o\\S*)\"\n",
    "        return re.match(pattern, self.model) is not None\n",
    "\n",
    "    def generate_text(self, messages: list[MessageLike], tools: Optional[set[Callable[..., str]]] = None) -> Generator[LLMStreamingEvent, None, None]:\n",
    "        \"\"\"Generate text based on the given prompt.\"\"\"\n",
    "        openai_tools: Sequence[ToolParam] = []\n",
    "        for t in tools or []:\n",
    "            wrapped_tool = tool(t, api=OpenAIAPIProtocol.RESPONSES)\n",
    "            openai_tools.append(cast(ToolParam, wrapped_tool.schema))\n",
    "        \n",
    "        generator = self.client.responses.create(\n",
    "            model=self.model,\n",
    "            input=messages,\n",
    "            tools=openai_tools,\n",
    "            parallel_tool_calls=self.allow_parallel_tool_calls,\n",
    "            reasoning=self.reasoning,\n",
    "            temperature=self.temperature,\n",
    "            top_p=self.top_p,\n",
    "            stream=True\n",
    "        )\n",
    "\n",
    "        final_response = \"\"\n",
    "        final_tool_calls: list[ToolCall] = []\n",
    "        for event in generator:\n",
    "            if isinstance(event, ResponseReasoningSummaryTextDeltaEvent):\n",
    "                yield LLMStreamingEvent(\n",
    "                    event=StreamingEventType.REASONING_TOKEN,\n",
    "                    data=event.delta\n",
    "                )\n",
    "            elif isinstance(event, ResponseTextDeltaEvent):\n",
    "                final_response += event.delta\n",
    "                yield LLMStreamingEvent(\n",
    "                    event=StreamingEventType.TOKEN,\n",
    "                    data=event.delta\n",
    "                )\n",
    "            elif isinstance(event, ResponseReasoningSummaryTextDoneEvent):\n",
    "                yield LLMStreamingEvent(\n",
    "                    event=StreamingEventType.REASONING,\n",
    "                    data=event.text\n",
    "                )\n",
    "            elif isinstance(event, ResponseCompletedEvent):\n",
    "                for output_item in event.response.output:\n",
    "                    if isinstance(output_item, ResponseFunctionToolCall):\n",
    "                        final_tool_calls.append(\n",
    "                            ToolCall(\n",
    "                                name=output_item.name,\n",
    "                                tool_call_id=output_item.call_id,\n",
    "                                arguments=json.loads(output_item.arguments)\n",
    "                            )\n",
    "                        )\n",
    "                if final_tool_calls:\n",
    "                    yield LLMStreamingEvent(\n",
    "                        event=StreamingEventType.TOOL_CALL,\n",
    "                        data=\"\",\n",
    "                        tool_calls=final_tool_calls\n",
    "                    )\n",
    "                else:\n",
    "                    yield LLMStreamingEvent(\n",
    "                        event=StreamingEventType.COMPLETED,\n",
    "                        data=final_response,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b513da36",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAILLMResponsesAPI(\n",
    "    client=client,\n",
    "    model=\"gpt-4o-mini\",\n",
    "    allow_parallel_tool_calls=True, \n",
    "    reasoning=Reasoning(effort=\"medium\", summary=\"auto\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0682021",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather(location: str) -> str:\n",
    "    return f\"The weather in {location} is sunny with a high of 75Â°F.\"\n",
    "\n",
    "def get_user_info(user_name: str) -> str:\n",
    "    return f\"User {user_name} is a software developer from San Francisco.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523cdd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for event in llm.generate_text([{\"role\": \"user\", \"content\": \"What's the difference between MCP Servers and traditional function calling API?\"}], tools=[get_weather, get_user_info]):\n",
    "#     print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412b0291",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "agent = Agent(\n",
    "    llm=llm, \n",
    "    tool_handler=ToolHandler(tools={get_weather, get_user_info})\n",
    ")\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "first_token_flag = True\n",
    "messages: list[MessageLike] = []\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"Enter your query (or 'exit' to quit): \")\n",
    "    if user_input.lower() == 'exit':\n",
    "        break\n",
    "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "    for event in agent.run(messages):\n",
    "        if first_token_flag:\n",
    "            elapsed = timeit.default_timer() - start_time\n",
    "            print(f\"Time to first token: {elapsed:.2f} seconds\")\n",
    "            first_token_flag = False\n",
    "        print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5b82c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dynamic-tool-router (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
